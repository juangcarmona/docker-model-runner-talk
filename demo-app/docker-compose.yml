services:
  openai-web-client:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8081:8081"
      - "5680:5680"
    environment:
      - PORT=8081
      - LLM_BASE_URL=http://host.docker.internal:12434/engines/llama.cpp/v1
      - LLM_MODEL_NAME=ai/smollm3:Q8_0
      - DEBUG=false
      - RUN_MODE=container
    models:
      - llama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

models:
  llama:
    model: ai/smollm3:Q8_0
    context_size: 2048
